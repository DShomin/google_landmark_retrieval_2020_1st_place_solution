{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "v2total_sample.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGL1gD-DvtLj",
        "outputId": "290fdeb4-e725-4df2-c19b-2ff2ab921d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qUW9WRf-dYR",
        "outputId": "5dab7ecf-757f-4428-eff2-0ce18ff67070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        }
      },
      "source": [
        "!pip install -q efficientnet\n",
        "import math, re, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow.keras.layers as L\n",
        "import tensorflow.keras.backend as K\n",
        "import efficientnet.tfkeras as efn\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn.model_selection import GroupKFold\n",
        "import pickle\n",
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██████▌                         | 10kB 25.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 30kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.6MB/s \n",
            "\u001b[?25hRunning on TPU  grpc://10.97.102.170:8470\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.97.102.170:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.97.102.170:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "REPLICAS:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4mZmnVB-qAA"
      },
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "IMAGE_SIZE = [512,512]\n",
        "EPOCHS = 2000\n",
        "NOT_CLEAN_WEIGHT = 0.5\n",
        "EMB_DIM=512\n",
        "BATCH_SIZE_PER_TPU = 4\n",
        "EFF_VER = 7\n",
        "BATCH_SIZE = BATCH_SIZE_PER_TPU * strategy.num_replicas_in_sync\n",
        "FOLDERNAME = 'v2total_sample'\n",
        "DRIVE_DS_PATH = '/content/gdrive/My Drive/landmark/'+FOLDERNAME\n",
        "os.makedirs(DRIVE_DS_PATH,exist_ok=True)\n",
        "NUM_CLASSES = 203094\n",
        "EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3,\n",
        "        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6,efn.EfficientNetB7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrgl_r2bopyw"
      },
      "source": [
        "train_total_16fold = pd.read_csv('/content/gdrive/My Drive/landmark/train_total_16fold.csv')\n",
        "train_16fold = pd.read_csv('/content/gdrive/My Drive/landmark/train_16fold.csv')\n",
        "cleanToTotal = tf.constant(sorted(list(set(train_16fold['landmark_id']))))\n",
        "from collections import Counter\n",
        "landmarkIdCounter = dict(Counter(train_total_16fold['landmark_id']))\n",
        "train_total_16fold['counts'] = [landmarkIdCounter[x] for x in train_total_16fold['landmark_id']]\n",
        "countIdList=[]\n",
        "for key in sorted(landmarkIdCounter):\n",
        "    countIdList.append(landmarkIdCounter[key])\n",
        "isCleanList = list(train_total_16fold['isClean'])\n",
        "countsList = list(train_total_16fold['counts'])\n",
        "isCleanLogCountWeightList = []\n",
        "for i in range(len(isCleanList)):\n",
        "    if isCleanList[i] == 0:\n",
        "        isCleanLogCountWeightList.append(NOT_CLEAN_WEIGHT * (1/np.log(countsList[i]+1)))\n",
        "    else:\n",
        "        isCleanLogCountWeightList.append(1 * (1/np.log(countsList[i]+1)))\n",
        "scaleV = (1/np.mean(isCleanLogCountWeightList))\n",
        "lossWeight =np.array(scaleV/np.log(np.array(countIdList)+1))\n",
        "lossWeight = tf.constant(lossWeight)\n",
        "lossWeight = tf.tile(tf.expand_dims(lossWeight,0),tf.constant([BATCH_SIZE_PER_TPU,1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHtbxnNP-u_I"
      },
      "source": [
        "#GCS_DS_PATH = 'GCS private bucket path'\n",
        "TRAIN_GCS_PATH = GCS_DS_PATH + '/v2total_tfrecord_train'\n",
        "TRAIN_FILENAMES = tf.io.gfile.glob(TRAIN_GCS_PATH + '/*.tfrec')\n",
        "VALID_GCS_PATH = GCS_DS_PATH + '/v2clean_tfrecord_valid'\n",
        "VALID_FILENAMES = tf.io.gfile.glob(VALID_GCS_PATH + '/*.tfrec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSvyETqAKWJ1",
        "outputId": "eb2f799f-24ab-46f5-a1c9-898bcb39b578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "def normalize_image(image):\n",
        "    image -= tf.constant([0.485 * 255, 0.456 * 255, 0.406 * 255])  # RGB\n",
        "    image /= tf.constant([0.229 * 255, 0.224 * 255, 0.225 * 255])  # RGB\n",
        "    return image\n",
        "\n",
        "def decode_image(image_data):\n",
        "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "    image = tf.image.resize(image, IMAGE_SIZE)\n",
        "    image = normalize_image(image)\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
        "    return image\n",
        "\n",
        "def img_aug(image, label, isClean):\n",
        "    img = tf.image.random_flip_left_right(image)\n",
        "    return img, label, isClean\n",
        "\n",
        "def read_labeled_tfrecord(example):\n",
        "    LABELED_TFREC_FORMAT = {\n",
        "        \"_bits\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "        \"_class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n",
        "        '_id': tf.io.FixedLenFeature([], tf.string),\n",
        "        '_isClean': tf.io.FixedLenFeature([], tf.int64)\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
        "    image = decode_image(example['_bits'])\n",
        "    label = tf.cast(example['_class'],tf.int32)\n",
        "    isClean = tf.cast(example['_isClean'], tf.int32)\n",
        "    return image, label, isClean\n",
        "\n",
        "def valid_read_labeled_tfrecord(example):\n",
        "    LABELED_TFREC_FORMAT = {\n",
        "        \"_bits\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "        \"_class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n",
        "        '_id': tf.io.FixedLenFeature([], tf.string)\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
        "    image = decode_image(example['_bits'])\n",
        "    label = tf.cast(example['_class'],tf.int32)\n",
        "    return image, cleanToTotal[label], tf.constant([0])\n",
        "\n",
        "def load_dataset(filenames, train=True,ordered=False):\n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False # disable order,increase speed\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
        "    if train==True:\n",
        "        dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
        "    else:\n",
        "        dataset = dataset.map(valid_read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
        "    return dataset\n",
        "\n",
        "def get_training_dataset():\n",
        "    dataset = load_dataset(TRAIN_FILENAMES,train=True,ordered=False)\n",
        "    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
        "    dataset = dataset.map(img_aug, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset\n",
        "\n",
        "def get_validation_dataset():\n",
        "    dataset = load_dataset(VALID_FILENAMES,train=False,ordered=True)\n",
        "    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset\n",
        "\n",
        "def count_data_items(filenames):\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    return np.sum(n)\n",
        "    \n",
        "NUM_TRAINING_IMAGES = count_data_items(TRAIN_FILENAMES)\n",
        "NUM_VALIDATION_IMAGES = count_data_items(VALID_FILENAMES)\n",
        "print('Dataset: {} training images'.format(NUM_TRAINING_IMAGES))\n",
        "print('Dataset: {} validation images'.format(NUM_VALIDATION_IMAGES))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 4060592 training images\n",
            "Dataset: 72322 validation images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaF69i92akLI"
      },
      "source": [
        "class ArcMarginProduct_v2(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ArcMarginProduct_v2, self).__init__()\n",
        "        self.num_classes= num_classes\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_variable(\n",
        "            \"weights\", shape=[int(input_shape[-1]), self.num_classes])\n",
        "    def call(self, input):\n",
        "        cosine = tf.matmul(tf.nn.l2_normalize(input, axis=1), tf.nn.l2_normalize(self.w, axis=0))\n",
        "        return cosine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRVjN7uffEl6"
      },
      "source": [
        "def getefn():\n",
        "    pretrained_model = EFNS[EFF_VER](weights=None, include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
        "    pretrained_model.trainable = True\n",
        "    return pretrained_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSOMg9lOaheI",
        "outputId": "0b101bcb-2996-435e-b1ed-b93439652077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "def ArcFaceResNet():\n",
        "    x= inputs = tf.keras.Input([*IMAGE_SIZE, 3], name='input_image')\n",
        "    x = getefn()(x)\n",
        "    x = L.GlobalAveragePooling2D()(x)\n",
        "    x = L.Dense(EMB_DIM, activation='swish')(x)\n",
        "    target = ArcMarginProduct_v2(NUM_CLASSES)(x)\n",
        "    return tf.keras.Model(inputs, target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef ArcFaceResNet():\\n    x= inputs = tf.keras.Input([*IMAGE_SIZE, 3], name='input_image')\\n    #x =  tf.keras.applications.ResNet101(weights='imagenet',include_top=False, input_shape=[*IMAGE_SIZE, 3])(x)\\n    #x = tf.keras.applications.ResNet101(weights='imagenet',include_top=False, input_shape=[*IMAGE_SIZE, 3])(x)\\n    x = getefn()(x)\\n    x = L.GlobalAveragePooling2D()(x)\\n    #x= GeMPoolingLayer(p=3., train_p=True)(x)\\n    #x= L.Dropout(0.0)(x)\\n    x = L.Dense(512, activation='swish')(x)\\n    #x = L.BatchNormalization()(x)\\n    target = ArcMarginProduct_v2(NUM_CLASSES)(x)\\n    return tf.keras.Model(inputs, target)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N03QSYmzVFrC"
      },
      "source": [
        "#references\n",
        "#https://arxiv.org/abs/1905.00292\n",
        "#https://github.com/taekwan-lee/adacos-tensorflow/blob/master/adacos.py\n",
        "class adacosLoss:\n",
        "    def __init__(self):\n",
        "        self.adacos_s = tf.math.sqrt(2.0) * tf.math.log(tf.cast(NUM_CLASSES - 1,tf.float32))\n",
        "        self.pi =  tf.constant(3.14159265358979323846)\n",
        "        self.theta_zero = self.pi/4\n",
        "        self.m = 0.5\n",
        "    def getLoss(self, labels, logits, cleans, mode):\n",
        "        mask = tf.one_hot(tf.cast(labels, tf.int32), depth = NUM_CLASSES)\n",
        "        theta = tf.math.acos(tf.clip_by_value(logits, -1.0 + 1e-7, 1.0 - 1e-7))\n",
        "        B_avg =tf.where(mask==1,tf.zeros_like(logits), tf.math.exp(self.adacos_s * logits))\n",
        "        B_avg = tf.reduce_mean(tf.reduce_sum(B_avg, axis=1), name='B_avg')\n",
        "        B_avg = tf.stop_gradient(B_avg)\n",
        "        theta_class = tf.gather_nd(theta, tf.stack([tf.range(tf.shape(labels)[0]), labels], axis=1), name='theta_class')\n",
        "        theta_med = tfp.stats.percentile(theta_class, q=50)\n",
        "        theta_med = tf.stop_gradient(theta_med)\n",
        "        self.adacos_s=(tf.math.log(B_avg) / tf.cos(tf.minimum(self.theta_zero, theta_med)))\n",
        "        output = tf.multiply(self.adacos_s, logits, name='adacos_logits')        \n",
        "        cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction=tf.keras.losses.Reduction.NONE)\n",
        "        if mode=='train':\n",
        "            isCleanValues = tf.cast(tf.gather_nd(tf.constant([[NOT_CLEAN_WEIGHT],[1.0]]), tf.stack([cleans,tf.tile(tf.constant([0]),tf.shape(cleans))], axis=1)), tf.float32)\n",
        "            weightValues = tf.cast(tf.gather_nd(lossWeight, tf.stack([tf.range(BATCH_SIZE_PER_TPU),labels], axis=1)),tf.float32)\n",
        "            loss = cce(labels, output, sample_weight = tf.multiply(isCleanValues, weightValues))\n",
        "        else :\n",
        "            loss = cce(labels, output)\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umdmOUGZK6cx",
        "outputId": "3bd95b66-8d02-46e6-ac58-b75cee86e180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "with strategy.scope():\n",
        "    model = ArcFaceResNet()\n",
        "    optimizer = tf.keras.optimizers.SGD(1e-3, momentum=0.9, decay = 1e-5)\n",
        "    train_loss = tf.keras.metrics.Sum()\n",
        "    valid_loss = tf.keras.metrics.Sum()\n",
        "    def loss_fn(labels, predictions, cleans, mode='train'):\n",
        "        _adacosLoss = adacosLoss()\n",
        "        per_example_loss = _adacosLoss.getLoss(labels, predictions, cleans, mode)\n",
        "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size= BATCH_SIZE)\n",
        "    model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-23-00762d988822>:13: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-23-00762d988822>:13: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_image (InputLayer)     [(None, 736, 736, 3)]     0         \n",
            "_________________________________________________________________\n",
            "efficientnet-b6 (Functional) (None, 23, 23, 2304)      40960136  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "arc_margin_product_v2 (ArcMa (None, 203094)            103984128 \n",
            "=================================================================\n",
            "Total params: 146,124,424\n",
            "Trainable params: 145,899,992\n",
            "Non-trainable params: 224,432\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3ZhDpZuLLxR"
      },
      "source": [
        "STEPS_PER_TPU_CALL = NUM_TRAINING_IMAGES // BATCH_SIZE // 8\n",
        "VALIDATION_STEPS_PER_TPU_CALL = NUM_VALIDATION_IMAGES // BATCH_SIZE\n",
        "@tf.function\n",
        "def train_step(data_iter):\n",
        "    def train_step_fn(images, labels, cleans):\n",
        "        with tf.GradientTape() as tape:\n",
        "            cosine = model(images)\n",
        "            loss = loss_fn(labels, cosine, cleans)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        #update metrics\n",
        "        train_loss.update_state(loss)\n",
        "    #this loop runs on the TPU\n",
        "    for _ in tf.range(STEPS_PER_TPU_CALL):\n",
        "        strategy.run(train_step_fn, next(data_iter))\n",
        "@tf.function\n",
        "def valid_step(data_iter):\n",
        "    def valid_step_fn(images, labels, cleans):\n",
        "        probabilities = model(images, training=False)\n",
        "        loss = loss_fn(labels, probabilities, cleans, 'valid')\n",
        "        # update metrics\n",
        "        valid_loss.update_state(loss)\n",
        "    # this loop runs on the TPU\n",
        "    for _ in tf.range(VALIDATION_STEPS_PER_TPU_CALL):\n",
        "        strategy.run(valid_step_fn, next(data_iter))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM2ib6haY4BQ",
        "outputId": "48272405-76f4-4628-e1eb-556cb0cab1b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "from collections import namedtuple\n",
        "start_time = epoch_start_time = time.time()\n",
        "train_dist_ds = strategy.experimental_distribute_dataset(get_training_dataset())\n",
        "valid_dist_ds = strategy.experimental_distribute_dataset(get_validation_dataset())\n",
        "print(\"Training steps per epoch:\", STEPS_PER_EPOCH, \"in increments of\", STEPS_PER_TPU_CALL)\n",
        "epoch = START_EPOCH\n",
        "train_data_iter = iter(train_dist_ds) # the training data iterator is repeated and it is not reset\n",
        "                                      # for each validation run (same as model.fit)\n",
        "valid_data_iter = iter(valid_dist_ds)\n",
        "while True:\n",
        "    train_step(train_data_iter)\n",
        "    print('|', end='', flush=True)\n",
        "    valid_step(valid_data_iter)\n",
        "    print('=', end='', flush=True)\n",
        "    trainLossV = train_loss.result().numpy()/STEPS_PER_TPU_CALL\n",
        "    print('\\nEPOCH {:d}/{:d}'.format(epoch+1, EPOCHS))\n",
        "    print('loss: {:0.4f}'.format(trainLossV),\n",
        "          'valid_loss : {:0.4f} '.format(valid_loss.result().numpy() / VALIDATION_STEPS_PER_TPU_CALL),\n",
        "          flush=True)\n",
        "    model.save_weights(os.path.join(DRIVE_DS_PATH, 'weights.epoch{:02d}.loss{:0.4f}.valid_loss{:0.4f}.hdf5').format(epoch+1, trainLossV,valid_loss.result().numpy() /VALIDATION_STEPS_PER_TPU_CALL))\n",
        "    epoch += 1\n",
        "    train_loss.reset_states()\n",
        "    valid_loss.reset_states()\n",
        "    if epoch >= EPOCHS:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training steps per epoch: 15861 in increments of 15861\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "|=\n",
            "EPOCH 63/2000\n",
            "time: 94.7s loss: 1.3050 valid_loss : 1.2217 \n",
            "0.001\n",
            "|=\n",
            "EPOCH 64/2000\n",
            "time: 0.1s loss: 1.3127 valid_loss : 1.2160 \n",
            "0.001\n",
            "|=\n",
            "EPOCH 65/2000\n",
            "time: 0.1s loss: 1.2752 valid_loss : 1.2279 \n",
            "0.001\n",
            "|=\n",
            "EPOCH 66/2000\n",
            "time: 0.1s loss: 1.3020 valid_loss : 1.2120 \n",
            "0.001\n",
            "|="
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
